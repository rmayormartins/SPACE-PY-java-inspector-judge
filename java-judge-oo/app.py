import javalang
from typing import Dict, List, Tuple
from dataclasses import dataclass
import gradio as gr

@dataclass
class RubricCriterion:
    name: str
    description: str
    weight: int
    is_essential: bool
    levels: Dict[str, Dict[str, float]]

class EnhancedJavaPOOEvaluator:
    """Avaliador POO com rubrica detalhada"""

    def __init__(self):
        self.rubric = {
            "classes_objects": RubricCriterion(
                name="Classes e Objetos",
                description="Avalia a defini√ß√£o e uso de classes e objetos",
                weight=20,
                is_essential=True,
                levels={
                    "Fraco": {"threshold": 0, "description": "Nenhuma ou poucas classes/objetos"},
                    "Regular": {"threshold": 10, "description": "Classes b√°sicas sem organiza√ß√£o clara"},
                    "Bom": {"threshold": 15, "description": "Classes bem estruturadas e objetos adequados"},
                    "Excelente": {"threshold": 20, "description": "Excelente uso de classes e objetos"}
                }
            ),
            "methods": RubricCriterion(
                name="M√©todos",
                description="Avalia m√©todos e sua organiza√ß√£o",
                weight=20,
                is_essential=True,
                levels={
                    "Fraco": {"threshold": 0, "description": "Poucos m√©todos ou mal estruturados"},
                    "Regular": {"threshold": 10, "description": "M√©todos b√°sicos sem sobrecarga"},
                    "Bom": {"threshold": 15, "description": "Boa organiza√ß√£o e alguns m√©todos sobrecarregados"},
                    "Excelente": {"threshold": 20, "description": "Excelente organiza√ß√£o e uso de sobrecarga"}
                }
            ),
            "attributes": RubricCriterion(
                name="Atributos",
                description="Avalia atributos e sua organiza√ß√£o",
                weight=20,
                is_essential=True,
                levels={
                    "Fraco": {"threshold": 0, "description": "Poucos atributos ou mal organizados"},
                    "Regular": {"threshold": 10, "description": "Atributos b√°sicos sem encapsulamento"},
                    "Bom": {"threshold": 15, "description": "Boa organiza√ß√£o de atributos"},
                    "Excelente": {"threshold": 20, "description": "Excelente organiza√ß√£o e encapsulamento"}
                }
            ),
            "encapsulation": RubricCriterion(
                name="Encapsulamento",
                description="Avalia uso de modificadores e getters/setters",
                weight=10,
                is_essential=False,
                levels={
                    "Ausente": {"threshold": 0, "description": "Sem encapsulamento"},
                    "Parcial": {"threshold": 5, "description": "Encapsulamento b√°sico"},
                    "Bom": {"threshold": 7.5, "description": "Bom uso de encapsulamento"},
                    "Excelente": {"threshold": 10, "description": "Encapsulamento completo e correto"}
                }
            ),
            "inheritance": RubricCriterion(
                name="Heran√ßa",
                description="Avalia uso de heran√ßa",
                weight=10,
                is_essential=False,
                levels={
                    "Ausente": {"threshold": 0, "description": "Sem uso de heran√ßa"},
                    "Parcial": {"threshold": 5, "description": "Uso b√°sico de heran√ßa"},
                    "Bom": {"threshold": 7.5, "description": "Bom uso de heran√ßa"},
                    "Excelente": {"threshold": 10, "description": "Uso avan√ßado e apropriado de heran√ßa"}
                }
            ),
            "polymorphism": RubricCriterion(
                name="Polimorfismo",
                description="Avalia uso de polimorfismo",
                weight=10,
                is_essential=False,
                levels={
                    "Ausente": {"threshold": 0, "description": "Sem uso de polimorfismo"},
                    "Parcial": {"threshold": 5, "description": "Uso b√°sico de sobrescrita"},
                    "Bom": {"threshold": 7.5, "description": "Bom uso de polimorfismo"},
                    "Excelente": {"threshold": 10, "description": "Uso avan√ßado de polimorfismo"}
                }
            ),
            "abstraction": RubricCriterion(
                name="Abstra√ß√£o",
                description="Avalia uso de abstra√ß√µes",
                weight=10,
                is_essential=False,
                levels={
                    "Ausente": {"threshold": 0, "description": "Sem uso de abstra√ß√£o"},
                    "Parcial": {"threshold": 5, "description": "Uso b√°sico de interfaces/classes abstratas"},
                    "Bom": {"threshold": 7.5, "description": "Bom uso de abstra√ß√£o"},
                    "Excelente": {"threshold": 10, "description": "Uso completo de abstra√ß√µes"}
                }
            )
        }

    def evaluate_criterion(self, criterion: RubricCriterion, analysis_result: Dict) -> Tuple[float, str, str]:
        """Avalia um crit√©rio espec√≠fico baseado nos resultados da an√°lise"""
        score = 0
        level = list(criterion.levels.keys())[0]  # N√≠vel mais baixo por padr√£o
        feedback = []

        if criterion.name == "Classes e Objetos":
            num_classes = len(analysis_result.get("classes", []))
            num_objects = len(analysis_result.get("objects", []))

            if num_classes >= 3 and num_objects >= 5:
                score = criterion.weight
                level = "Excelente"
            elif num_classes >= 2 and num_objects >= 3:
                score = criterion.weight * 0.75
                level = "Bom"
            elif num_classes >= 1 and num_objects >= 1:
                score = criterion.weight * 0.5
                level = "Regular"

            feedback.append(f"Encontradas {num_classes} classes e {num_objects} objetos")

        elif criterion.name == "M√©todos":
            methods = analysis_result.get("methods", [])
            method_names = [m.name for m in methods]
            overloaded = len([name for name in method_names if method_names.count(name) > 1])

            if len(methods) >= 5 and overloaded >= 2:
                score = criterion.weight
                level = "Excelente"
            elif len(methods) >= 3 and overloaded >= 1:
                score = criterion.weight * 0.75
                level = "Bom"
            elif len(methods) >= 1:
                score = criterion.weight * 0.5
                level = "Regular"

            feedback.append(f"Encontrados {len(methods)} m√©todos, sendo {overloaded} sobrecarregados")

        elif criterion.name == "Atributos":
            attributes = analysis_result.get("attributes", [])
            num_private = analysis_result["encapsulation"]["private_count"]

            if len(attributes) >= 5 and num_private >= 3:
                score = criterion.weight
                level = "Excelente"
            elif len(attributes) >= 3 and num_private >= 1:
                score = criterion.weight * 0.75
                level = "Bom"
            elif len(attributes) >= 1:
                score = criterion.weight * 0.5
                level = "Regular"

            feedback.append(f"Encontrados {len(attributes)} atributos, sendo {num_private} privados")

        elif criterion.name == "Encapsulamento":
            num_private = analysis_result["encapsulation"]["private_count"]
            num_getters_setters = analysis_result["encapsulation"]["getters_setters"]

            if num_private >= 3 and num_getters_setters >= 4:
                score = criterion.weight
                level = "Excelente"
            elif num_private >= 2 and num_getters_setters >= 3:
                score = criterion.weight * 0.75
                level = "Bom"
            elif num_private >= 1 and num_getters_setters >= 2:
                score = criterion.weight * 0.5
                level = "Parcial"

            feedback.append(f"Encontrados {num_private} atributos privados e {num_getters_setters} getters/setters")

        elif criterion.name == "Heran√ßa":
            subclasses = analysis_result["inheritance"]["subclasses"]

            if len(subclasses) >= 3:
                score = criterion.weight
                level = "Excelente"
            elif len(subclasses) >= 2:
                score = criterion.weight * 0.75
                level = "Bom"
            elif len(subclasses) >= 1:
                score = criterion.weight * 0.5
                level = "Parcial"

            feedback.append(f"Encontradas {len(subclasses)} classes que usam heran√ßa")

        elif criterion.name == "Polimorfismo":
            overridden = len(analysis_result["polymorphism"]["overridden_methods"])

            if overridden >= 3:
                score = criterion.weight
                level = "Excelente"
            elif overridden >= 2:
                score = criterion.weight * 0.75
                level = "Bom"
            elif overridden >= 1:
                score = criterion.weight * 0.5
                level = "Parcial"

            feedback.append(f"Encontrados {overridden} m√©todos sobrescritos")

        elif criterion.name == "Abstra√ß√£o":
            abstract_classes = len(analysis_result["abstraction"]["abstract_classes"])
            interfaces = len(analysis_result["abstraction"]["interfaces"])

            if abstract_classes >= 1 and interfaces >= 1:
                score = criterion.weight
                level = "Excelente"
            elif abstract_classes >= 1 and interfaces >= 0:
                score = criterion.weight * 0.75
                level = "Bom"
            elif abstract_classes >= 1 or interfaces >= 1:
                score = criterion.weight * 0.5
                level = "Parcial"

            feedback.append(f"Encontradas {abstract_classes} classes abstratas e {interfaces} interfaces")

        return score, level, ". ".join(feedback)

    def analyze_code(self, code: str) -> Dict:
        """Analisa o c√≥digo Java e retorna dados brutos"""
        analysis = {
            "classes": [],
            "objects": [],
            "methods": [],
            "attributes": [],
            "encapsulation": {"private_count": 0, "getters_setters": 0},
            "inheritance": {"subclasses": []},
            "polymorphism": {"overridden_methods": []},
            "abstraction": {"abstract_classes": [], "interfaces": []}
        }

        try:
            tree = javalang.parse.parse(code)

            # An√°lise de classes e objetos
            analysis["classes"] = [node for _, node in tree.filter(javalang.tree.ClassDeclaration)]
            analysis["objects"] = [node for _, node in tree.filter(javalang.tree.VariableDeclarator)
                                 if isinstance(node.initializer, javalang.tree.ClassCreator)]

            # An√°lise de m√©todos
            analysis["methods"] = [node for _, node in tree.filter(javalang.tree.MethodDeclaration)]

            # An√°lise de atributos e encapsulamento
            fields = [node for _, node in tree.filter(javalang.tree.FieldDeclaration)]
            analysis["attributes"] = fields
            analysis["encapsulation"]["private_count"] = sum(1 for field in fields
                                                           if "private" in field.modifiers)

            # Contagem de getters e setters
            methods = analysis["methods"]
            getters_setters = sum(1 for method in methods
                                if method.name.startswith('get') or method.name.startswith('set'))
            analysis["encapsulation"]["getters_setters"] = getters_setters

            # An√°lise de heran√ßa
            analysis["inheritance"]["subclasses"] = [cls for cls in analysis["classes"]
                                                   if cls.extends is not None]

            # An√°lise de polimorfismo
            analysis["polymorphism"]["overridden_methods"] = [method for method in methods
                                                            if any(ann.name == "Override"
                                                                  for ann in (method.annotations or []))]

            # An√°lise de abstra√ß√£o
            analysis["abstraction"]["abstract_classes"] = [cls for cls in analysis["classes"]
                                                         if "abstract" in cls.modifiers]
            analysis["abstraction"]["interfaces"] = [node for _, node in tree.filter(javalang.tree.InterfaceDeclaration)]

        except Exception as e:
            print(f"Erro na an√°lise: {str(e)}")

        return analysis

    def evaluate_code(self, code: str) -> Dict:
        """Avalia o c√≥digo Java usando a rubrica detalhada"""
        analysis = self.analyze_code(code)
        evaluation = {
            "scores": {},
            "levels": {},
            "feedback": {},
            "summary": {
                "essential_score": 0,
                "bonus_score": 0,
                "total_score": 0
            }
        }

        # Avalia cada crit√©rio
        for criterion_key, criterion in self.rubric.items():
            score, level, feedback = self.evaluate_criterion(criterion, analysis)

            evaluation["scores"][criterion_key] = score
            evaluation["levels"][criterion_key] = level
            evaluation["feedback"][criterion_key] = feedback

            if criterion.is_essential:
                evaluation["summary"]["essential_score"] += score
            else:
                evaluation["summary"]["bonus_score"] += score

        evaluation["summary"]["total_score"] = min(100,
            evaluation["summary"]["essential_score"] +
            evaluation["summary"]["bonus_score"])

        # Determina n√≠vel geral
        if evaluation["summary"]["total_score"] >= 90:
            evaluation["summary"]["proficiency"] = "Excelente"
        elif evaluation["summary"]["total_score"] >= 75:
            evaluation["summary"]["proficiency"] = "Bom"
        elif evaluation["summary"]["total_score"] >= 60:
            evaluation["summary"]["proficiency"] = "Satisfat√≥rio"
        else:
            evaluation["summary"]["proficiency"] = "Necessita Melhorias"

        return evaluation

# Interface Gradio
with gr.Blocks(title="Avaliador de POO em Java") as demo:
    gr.Markdown("# Avaliador de POO em Java")
    gr.Markdown("""
    Este avaliador analisa c√≥digo Java em rela√ß√£o aos princ√≠pios de Programa√ß√£o Orientada a Objetos.

    Crit√©rios avaliados:
   
    """)
    # Links usando caminho completo do Hugging Face
    gr.HTML(f"""
    <h3>
        <a href="https://huggingface.co/spaces/rmayormartins/java-judge-oo/resolve/main/assets/rubric.pdf" target="_blank">üìÑ Visualizar Rubrica PDF</a>
    </h3>
    <h3>
        <a href="https://huggingface.co/spaces/rmayormartins/java-judge-oo/resolve/main/assets/rubric_table.PNG" target="_blank">üìä Visualizar Tabela da Rubrica</a>
    </h3>
    """)

    upload = gr.File(label="Carregue arquivos Java para avalia√ß√£o", file_types=[".java"], file_count="multiple")
    evaluate_button = gr.Button("Avaliar C√≥digo")
    output = gr.Textbox(label="Resultado da Avalia√ß√£o", lines=25)

    def evaluate_code_files(files) -> str:
        """Fun√ß√£o para avaliar m√∫ltiplos arquivos Java"""
        evaluator = EnhancedJavaPOOEvaluator()
        results = []

        for file in files:
            with open(file.name, 'r', encoding='utf-8') as f:
                code = f.read()
            evaluation = evaluator.evaluate_code(code)

            # Formatar resultado por arquivo
            result = f"\n{'='*50}\nAvalia√ß√£o do arquivo: {file.name}\n{'='*50}\n\n"

            # Pontua√ß√£o e n√≠vel geral
            result += f"Pontua√ß√£o Total: {evaluation['summary']['total_score']:.1f}/100\n"
            result += f"N√≠vel de Profici√™ncia: {evaluation['summary']['proficiency']}\n"
            result += f"Pontua√ß√£o Essencial: {evaluation['summary']['essential_score']:.1f}/60\n"
            result += f"Pontua√ß√£o B√¥nus: {evaluation['summary']['bonus_score']:.1f}/40\n\n"

            # Detalhamento por crit√©rio
            result += "Avalia√ß√£o Detalhada por Crit√©rio:\n"
            result += "-" * 30 + "\n\n"

            for criterion_key, criterion in evaluator.rubric.items():
                result += f"‚Ä¢ {criterion.name}:\n"
                result += f"  N√≠vel: {evaluation['levels'][criterion_key]}\n"
                result += f"  Pontua√ß√£o: {evaluation['scores'][criterion_key]:.1f}/{criterion.weight}\n"
                if evaluation['feedback'][criterion_key]:
                    result += f"  Feedback: {evaluation['feedback'][criterion_key]}\n"
                result += "\n"

            results.append(result)

        return "\n".join(results)

    evaluate_button.click(fn=evaluate_code_files, inputs=upload, outputs=output)

if __name__ == "__main__":
    demo.launch(debug=True)
